# ============================================================================
# JEPA GNN Training with SimSiam (Mode 4: Joint Training from Start)
# ============================================================================
# Uses GNN (bipartite graph) architecture with JEPA self-supervised learning.
# SimSiam mode: Shared encoder with stop-gradient (no separate target encoder).
# Mode 4: Joint KKT+JEPA training from epoch 0 (no pre-training phase).
#
# This configuration is more memory-efficient than EMA mode and suitable for
# production deployments or resource-constrained environments.
#
# Usage: python train.py --config configs/config_jepa_gnn_simsiam.yml
# ============================================================================

data:
  # Problem types to generate/train on
  problems: ["CA"]              # Combinatorial Auction problems
  is_sizes: [2]                 # Independent Set sizes
  ca_sizes: [5]                 # CA problem sizes (5 items)
  sc_sizes: [2]                 # Set Cover sizes
  cfl_sizes: [5]                # Capacitated Facility Location sizes
  rnd_sizes: [2]                # Random LP sizes

  # Data splits
  test_split: 0.15              # 15% for test set
  val_split: 0.15               # 15% for validation set
  # Remaining 70% for training

  # Dataset size
  n_instances: 7000             # Total instances to generate

  # Paths
  data_root: ./data/instances   # Root directory for generated instances

  # Solver settings
  solve: true                   # Solve instances with Gurobi to get optimal solutions
  gurobi_threads: 4             # Number of threads for Gurobi solver

  # System
  n_jobs: 32                    # Parallel jobs for data generation

# GNN architecture settings (USED for this configuration)
gnn_policy:
  embedding_size: 128           # Dimension of node embeddings
  cons_nfeats: 4                # Number of constraint node features
  edge_nfeats: 1                # Number of edge features (coefficient values)
  var_nfeats: 18                # Number of variable node features
  num_emb_type: periodic        # Numeric embedding type: "periodic" (recommended), "pwl", or "linear"
  num_emb_bins: 32              # Bins for piecewise-linear embeddings
  num_emb_freqs: 16             # Frequencies for periodic embeddings (used with periodic type)

# Training settings
training:
  # Model architecture
  use_bipartite_graphs: true    # true = GNN (this config), false = MLP
                                 # GNN represents LP as bipartite graph:
                                 # - Constraint nodes (one per Ax ≤ b constraint)
                                 # - Variable nodes (one per decision variable x_j)
                                 # - Edges connect constraint i to variable j if A[i,j] ≠ 0

  # Hardware
  devices: "0"                  # GPU device IDs (comma-separated)

  # Optimization
  batch_size: 128               # Batch size for training (reduced for GNN, more memory intensive)
  epochs: 200                   # Total training epochs
  lr: 0.001                     # Learning rate (Adam optimizer)
  seed: 42                      # Random seed for reproducibility

  # KKT loss weights
  primal_weight: 0.1                        # Primal feasibility: Ax ≤ b
  dual_weight: 0.1                          # Dual feasibility: λ ≥ 0
  stationarity_weight: 0.6                  # Stationarity: c + A^T λ = 0
  complementary_slackness_weight: 0.2       # Complementary slackness: λ·(Ax-b) = 0

  # Logging
  num_workers: 0                # DataLoader workers (0 = main process only)
  log_every: 5                  # Log metrics every N batches

  # =========================================================================
  # JEPA: ENABLED with SimSiam mode
  # =========================================================================
  use_jepa: true                # Enable JEPA self-supervised learning

  # JEPA mode selection
  jepa_mode: "simsiam"          # SimSiam: Shared encoder with stop-gradient
                                 # No separate target encoder (memory efficient)
                                 # Encoder weights shared between online and target paths
                                 # Target path uses detach() for stop-gradient
                                 # Sufficient for large models and standard problems

  # Training schedule (Mode 4: Joint from start)
  jepa_pretrain_epochs: 0       # No pre-training phase
                                 # Training combines KKT + JEPA from epoch 1
                                 # Faster to converge but may learn less general representations

  jepa_weight: 0.2              # Weight for JEPA loss
                                 # Total loss = KKT_loss + 0.2 * JEPA_loss
                                 # JEPA helps learn better representations while solving KKT

  # =========================================================================
  # GNN Node Masking Strategy
  # =========================================================================
  # For GNN, JEPA uses simple node-level masking:
  # - Randomly mask a fraction of constraint node features → zero vector
  # - Randomly mask a fraction of variable node features → zero vector
  # - Graph structure (edges, edge_attr) remains unchanged
  # - Target view is the original graph (clean, no masking)

  jepa_mask_ratio_nodes: 0.3    # 30% of constraint and variable nodes masked
                                 # Applied independently to each node type
                                 # Higher ratio = more challenging task, may learn better
                                 # Lower ratio = easier task, faster convergence
                                 # 0.3 is a good balance for most problems

  # =========================================================================
  # MLP masking (not used for GNN, but included for completeness)
  # =========================================================================
  # These settings are ignored when use_bipartite_graphs=true
  jepa_mask_entry_online: 0.40
  jepa_mask_row_online: 0.20
  jepa_mask_col_online: 0.20
  jepa_mask_entry_target: 0.10
  jepa_mask_row_target: 0.05
  jepa_mask_col_target: 0.05

  # =========================================================================
  # Augmentation Options
  # =========================================================================
  jepa_noisy_mask: false        # false: Use zeros at masked node features
                                 # true: Add Gaussian noise to masked features
                                 # For GNN, noisy masking is less common (zeros work well)

  jepa_row_scaling: false       # false: No row scaling
                                 # true: Scale constraints (not typically used for GNN)
                                 # Row scaling is more relevant for MLP/matrix representation

  # =========================================================================
  # EMA Settings (NOT USED in SimSiam mode)
  # =========================================================================
  ema_momentum: 0.996           # Ignored in SimSiam mode
                                 # SimSiam does not use EMA - encoder is shared
                                 # Target embeddings use stop-gradient (detach()) instead
                                 # This makes SimSiam more memory-efficient

# Testing/evaluation settings
testing:
  ckpt: exps/kkt_jepa_gnn_simsiam/best.pt   # Path to checkpoint for evaluation
  file_path: data/instances/CA/instance/test/5/CA-5-0001.lp  # Single instance for testing
  out_dir: outputs                           # Output directory for test results
  device: "0"                                # GPU device for testing

# ============================================================================
# Expected Behavior
# ============================================================================
# All Epochs:   Joint KKT + JEPA training from start
#               train/loss_kkt:  Task-specific KKT loss
#               train/loss_jepa: Self-supervised node prediction loss
#               train/loss:      Combined loss = KKT + 0.2*JEPA
#
# Checkpoints:  Saves only online_model state_dict
#               No separate target_model (encoder is shared)
#               Smaller checkpoint files than EMA mode
#
# Memory:       ~1.2x baseline (only predictor overhead, no target encoder)
# Training:     Slightly slower than baseline due to dual forward passes
#               Faster than EMA mode (no EMA updates)
# Results:      Good performance, especially on large-scale problems
#               May be slightly worse than EMA mode on small datasets
#
# Advantages:   - Memory efficient (no separate target encoder)
#               - Faster training (no EMA updates)
#               - Simpler implementation (shared encoder)
#               - Good for production/deployment
#
# When to use:  - Limited GPU memory
#               - Large models
#               - Production deployments
#               - Resource-constrained environments
# ============================================================================
